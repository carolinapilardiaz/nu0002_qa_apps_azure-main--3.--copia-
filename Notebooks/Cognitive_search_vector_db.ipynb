{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840baf44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.0.299\n",
      "  Downloading langchain-0.0.299-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting anyio<4.0\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (2.8.6)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (8.2.3)\n",
      "Collecting langsmith<0.1.0,>=0.0.38\n",
      "  Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (3.8.5)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (2.31.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (1.26.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (0.5.14)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (1.10.12)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (2.0.21)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (23.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.299) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.299) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.299) (1.1.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (3.20.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.299) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.299) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.299) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.299) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.299) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (1.0.0)\n",
      "Installing collected packages: jsonpatch, anyio, langsmith, langchain\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.187\n",
      "    Uninstalling langchain-0.0.187:\n",
      "      Successfully uninstalled langchain-0.0.187\n",
      "Successfully installed anyio-3.7.1 jsonpatch-1.33 langchain-0.0.299 langsmith-0.0.41\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting azure-search-documents==11.4.0b8\n",
      "  Downloading azure_search_documents-11.4.0b8-py3-none-any.whl (305 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.6/305.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.10/site-packages (from azure-search-documents==11.4.0b8) (0.6.1)\n",
      "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.10/site-packages (from azure-search-documents==11.4.0b8) (1.1.28)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/site-packages (from azure-search-documents==11.4.0b8) (1.29.4)\n",
      "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (2023.7.22)\n",
      "Installing collected packages: azure-search-documents\n",
      "  Attempting uninstall: azure-search-documents\n",
      "    Found existing installation: azure-search-documents 11.4.0b3\n",
      "    Uninstalling azure-search-documents-11.4.0b3:\n",
      "      Successfully uninstalled azure-search-documents-11.4.0b3\n",
      "Successfully installed azure-search-documents-11.4.0b8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain==0.0.299\n",
    "!pip install -U azure-search-documents==11.4.0b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "537297d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99542a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802fd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    ScoringProfile,\n",
    "    TextWeights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2743f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b52efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_type = \"azure\"\n",
    "api_base_url = \"https://openaidemonubiral.openai.azure.com/\"\n",
    "api_version = \"2023-03-15-preview\"\n",
    "azure_api_key = \"ff5c606c134e4d1dae3426a412df834a\"\n",
    "\n",
    "openai.api_type = api_type\n",
    "openai.api_base = api_base_url\n",
    "openai.api_version = api_version\n",
    "openai.api_key = azure_api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = api_base_url\n",
    "os.environ[\"OPENAI_API_KEY\"] = azure_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f6b01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/langchain/embeddings/openai.py:217: UserWarning: WARNING! deployment_id is not default parameter.\n",
      "                    deployment_id was transferred to model_kwargs.\n",
      "                    Please confirm that deployment_id is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings_azure = OpenAIEmbeddings(deployment_id=\"text-embedding-ada-002\", chunk_size=1)\n",
    "\n",
    "llm_azure_chatgpt = AzureChatOpenAI(\n",
    "    deployment_name=\"nubiral-lab-01\", \n",
    "    temperature=0, \n",
    "    openai_api_version=api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6f444d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding = embeddings_azure.embed_query('test de emmbeding')\n",
    "len(test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b07b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c081f4",
   "metadata": {},
   "source": [
    "# TEST BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c450dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_address: str = \"https://cognitive-searchdemo.search.windows.net\"\n",
    "vector_store_password: str = \"Ndqr05h9fitPrfEh9GxNBb2vsvelw9oUgIew1VpUvYAzSeD2n9Zi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9db543",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name: str = \"vector-demo-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd92763",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_cognitive_search: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings_azure.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041891d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19dff406",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "pages = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1edede92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add799f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eea7d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YTUzODExMTAtMjQwNy00OThhLTg2MDEtNDVjOTQxMzRhMzUx',\n",
       " 'YzkzYmUxZTAtZjdlMC00MmE2LTk5ZWItMWY5MDg2OGI5ZWNi',\n",
       " 'N2Q1ZGE1MDctYWM1YS00MGZhLWI1MTEtNGExZjczODZhZTdk',\n",
       " 'MDZjYTdkYTItZWIwNS00ZDlhLTkzZWMtMDBjM2VjNzNhY2Y4',\n",
       " 'NmM4NzQ0MjAtMjljYS00MDJiLTg4OTktN2IwNThiMjg0NzYy',\n",
       " 'MjI1M2Y0ZTAtYTQ0YS00ODcxLWJhNmItMTA1ZTY3MjgwMzAy',\n",
       " 'Yjg1YTkxMjEtZmRlNS00NjM0LTliMzUtODQ1MTdlNDFlZGM2',\n",
       " 'YjcwZDEyMjgtZGMxMC00NDNiLWIzZTItN2IxNDM3OTk0ODdk',\n",
       " 'Mzc0MjYwYWYtMzU5Ni00YjRhLThkZDctMjk4NTNjYzFkYmZl',\n",
       " 'MzNjMmJkMGMtYzYzMi00ZDAyLWEzMjAtNGNiYzg2Y2MyOTQ3',\n",
       " 'ODM5NTBkNGUtNThmMC00NDU2LTkwYjAtZDVkMDY1ZTQ2N2Yy',\n",
       " 'OTRlZDk1OTAtNGJiYy00NGQ5LTk2MjQtZWYwZWY0MDRlMjkz',\n",
       " 'YTZkOTE0NWItMDgwMi00NWM1LTg2NTUtY2RmY2I3NDk3NWZh',\n",
       " 'ZDliY2JkNjktOTk5Ni00ZWY5LWIzZWQtM2EyNGQ0YWE0MGVj',\n",
       " 'MjQwOTYxNGQtYWYxNy00ZjJjLTkyZjQtNzgzNjgzMTYyODNk',\n",
       " 'N2YyZjM0NTYtNjRhMi00OWZkLWJiY2EtZGZlZTk3Y2FiZDk4',\n",
       " 'NzYyOTk1YWEtYTgyZS00NmUxLWE0ZmUtODgyYmQ1NzdhNDEx',\n",
       " 'YzkwMzBmZWQtYjI3Ni00ZDRlLWIwZmYtNmRhOTBkYzhlYjNm',\n",
       " 'MThmOTA1ZGEtMDQwMC00OGVmLWFmODUtM2QxNWJiZTdkNjE0',\n",
       " 'Y2ZjMWJkNDEtMmIyMC00MmNhLTlmNzktYWQ0Nzg2OTJhNWEz',\n",
       " 'NWU1NjVkYTktMDI1Ni00NGVjLWIwYWItOWU2Njk2Y2ZhYTk3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_cognitive_search.add_documents(documents=pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2bf91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7da1f0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='controladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales . \\n \\nÍndice de Términos - Deep Q Learning, péndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI ÓN \\n \\n esde hace  varias  décadas la  respuesta  dinámica y el control \\nde sistemas basados en n-péndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teoría de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemático preciso, además presentan \\nmúltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raíz de estos \\nproblemas surge la motivación de implementar  otras técnicas  como el  \\naprendizaje reforzado y aplicarlas al ámbito del control , debido a l', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='qué acción tomar bajo qué circunstancias , este algoritmo aprende el Implementación de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinámicos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumialán  Borja , C. H. Rodríguez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingeniería en Automatización  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el péndulo invertido estable.', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinámica del sistema se \\npresentan en la ecuación 3 y 4. \\n \\nΘ̈𝑡=𝑔sinΘ𝑡 + cos Θ𝑡 [−𝐹𝑡−𝑚𝐿Θ̇𝑡2sin Θ𝑡+𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚]−𝑢𝑝Θ̇𝑡\\n𝑚𝐿\\n𝐿 [4\\n3−𝑚cos 2Θ𝑡\\n𝑀+𝑚]                    (3) \\n \\n𝑋̈𝑡=𝐹𝑡+𝑚𝐿[Θ̇𝑡sinΘ𝑡−Θ̈𝑡cos Θ𝑡]−𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acción se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura está basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenó en un entorno en el cual se \\nrecibe información referente a la posición del péndulo, la velocidad', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='no lineal ( péndulo invertido ) como el entorno de interacción para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acción de acuerdo con el estado del sistema, \\nla acción incremental puede acercarse gradualmente a la \\nestrategia óptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacción . \\nPara lograr la convergencia del movimiento de un péndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicación del aprendizaje automático al \\ncontrol de sistemas complejos y caóticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales .', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_cognitive_search.similarity_search(\n",
    "    query=\"quienes son los autores?\",\n",
    "    k=5,\n",
    "    search_type=\"similarity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbe5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fed4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bbfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c660abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_db = vector_store_cognitive_search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cfa4b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='entorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el péndulo invertido estable.', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='controladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales . \\n \\nÍndice de Términos - Deep Q Learning, péndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI ÓN \\n \\n esde hace  varias  décadas la  respuesta  dinámica y el control \\nde sistemas basados en n-péndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teoría de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemático preciso, además presentan \\nmúltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raíz de estos \\nproblemas surge la motivación de implementar  otras técnicas  como el  \\naprendizaje reforzado y aplicarlas al ámbito del control , debido a l', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='no lineal ( péndulo invertido ) como el entorno de interacción para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acción de acuerdo con el estado del sistema, \\nla acción incremental puede acercarse gradualmente a la \\nestrategia óptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacción . \\nPara lograr la convergencia del movimiento de un péndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicación del aprendizaje automático al \\ncontrol de sistemas complejos y caóticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales .', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinámica del sistema se \\npresentan en la ecuación 3 y 4. \\n \\nΘ̈𝑡=𝑔sinΘ𝑡 + cos Θ𝑡 [−𝐹𝑡−𝑚𝐿Θ̇𝑡2sin Θ𝑡+𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚]−𝑢𝑝Θ̇𝑡\\n𝑚𝐿\\n𝐿 [4\\n3−𝑚cos 2Θ𝑡\\n𝑀+𝑚]                    (3) \\n \\n𝑋̈𝑡=𝐹𝑡+𝑚𝐿[Θ̇𝑡sinΘ𝑡−Θ̈𝑡cos Θ𝑡]−𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acción se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura está basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenó en un entorno en el cual se \\nrecibe información referente a la posición del péndulo, la velocidad', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='cabe resaltar que en todas las arquitecturas probadas cada capa oculta \\ncuenta con 24 neuronas.  \\n \\n TABLA V \\nARQUITECTURAS EMPLEADAS  \\n Capas ocultas  Cantidad de parametros  \\nModelo 1  1 170 \\nModelo 2  2 770 \\nModelo 3  3 1370  \\nModelo 4  4 1970  \\n \\n   En la figura 4 se presenta de  modo ejemplo una representación \\ngráfica del modelo 2 , con cuatro entradas y dos salidas.  \\n \\n \\nFig. 4. Diagrama de la red neuronal implementada  en el modelo 2 . \\nIV. RESULTADOS  PARCIALES  \\n   Se obtuvo 4 modelos los cuales fueron sometidos a un entrenamiento  \\nde 300 episodios en los cuales los modelos interactuaron con el \\nentorn o, la evolución del rendimiento de los  modelo s puede ser \\nobservada en la figura 5, en el cual se ve cómo  los modelo s mejora n \\ncontinuamente  el valor de recompensa a medida que son expuesto s al \\nentorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_db.get_relevant_documents(\"quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708ac8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900910af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a8ee7a",
   "metadata": {},
   "source": [
    "# TEST BUSQUEDA HIBRIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "656254a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_address: str = \"https://cognitive-searchdemo.search.windows.net\"\n",
    "vector_store_password: str = \"Ndqr05h9fitPrfEh9GxNBb2vsvelw9oUgIew1VpUvYAzSeD2n9Zi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "912493ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name: str = \"document-session-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdaaffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcd827ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        filterable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(embeddings_azure.embed_query('test de emmbeding')),\n",
    "        vector_search_configuration=\"default\",\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    # Additional field to store the session\n",
    "    SearchableField(\n",
    "        name=\"session\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        filterable=True,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2e48b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings_azure.embed_query,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad2fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f68369c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "pages = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e92a3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_to_document_metadata(documents, field_name, field_value):\n",
    "    \n",
    "    for document in documents:\n",
    "        \n",
    "        document.metadata[field_name] = field_value\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e322852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = add_data_to_document_metadata(pages, 'session', 'test_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc843ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='no lineal ( péndulo invertido ) como el entorno de interacción para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acción de acuerdo con el estado del sistema, \\nla acción incremental puede acercarse gradualmente a la \\nestrategia óptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacción . \\nPara lograr la convergencia del movimiento de un péndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicación del aprendizaje automático al \\ncontrol de sistemas complejos y caóticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales .', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28f53942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZmFiM2ZjOTMtMThjNS00MDVlLWJmMmYtZTUwOTY5ZDFjN2Fk',\n",
       " 'MTMxMDVmYzMtM2Y1NS00NGI3LTg3MGQtMWMzMGVhMzRjNjRh',\n",
       " 'ZTRkNmFhOTItMzU4Zi00ZDhlLWExOTUtM2RjYzAzMzA5YWM3',\n",
       " 'ZDM1OTI2NTEtOTgyOC00MjU0LTkxOTAtNjUzNjZmZTI2ZmM3',\n",
       " 'OWIyZmIzMzAtYmM1OC00NzNmLTk5YzMtODViMWY5YzU0NTkz',\n",
       " 'YWI3MjQ3NzItMTNlMi00ZDZhLWIyZWMtODBmNDgyZjljNWU1',\n",
       " 'YWQxZmJmMmQtNGQ3Ni00MjVlLWIxMjUtNzUxNzNiM2I3ZmQy',\n",
       " 'OGNjYjZjMGEtZDJiNC00ZDYxLWE1NDUtMjA3NDJjYTMyN2Qy',\n",
       " 'ZTk4MDQ4ZDAtNGI0Yy00OWVmLTgwMmUtNmFmMTE0MmZhZGEz',\n",
       " 'YjRkMGM4MDgtN2IwMi00NTg3LWFhZDYtOTAzZTFhNzE4YjFl',\n",
       " 'ZDRlMzE0ZjAtNjE2NS00MjNlLTk5ZjEtNmUyOTkyYmQ1YjUy',\n",
       " 'NDczZDdmNjItZjcwYy00YTc4LWFjMTctMmVhOTc0ZDc3ZTY3',\n",
       " 'MjNlZDk0NGEtM2JmZC00NWY0LTg1MmItZDg1NzhiNmRiMTZk',\n",
       " 'MDc4ZjIyMzQtNGI1Ny00N2UyLTllODctMjg4NTA4NjNmMGFm',\n",
       " 'ODc1YmY0NDUtYmFmNy00YmMwLTgzN2YtMDVjYmY3N2U0ZTJi',\n",
       " 'YmJhY2VkOGUtMDYxZi00NTljLTljYzAtMTVhOTM5NTVhYzNm',\n",
       " 'NmExZTViYTMtY2ZhNC00MzQ5LWE0MTAtMmI4Mzg4YjYyNGJj',\n",
       " 'MWM3MWFiN2EtZGU5MC00MDM0LWFiZjEtOTFhNmY4MTQzYmIx',\n",
       " 'NGEwODFlZjktZDBlNy00ZmZkLWJhNjctMDkzNjY1MDZkZmE1',\n",
       " 'YTE1MWQyZDktMTBhNi00M2YyLWJmNWYtZWMwOWFiNjg3N2Fl',\n",
       " 'ZjEzNWYwNDktY2I3MS00NmUwLWJmZjEtMzNlOGJjNDk3ZTI3']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b5c7bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinámica del sistema se \\npresentan en la ecuación 3 y 4. \\n \\nΘ̈𝑡=𝑔sinΘ𝑡 + cos Θ𝑡 [−𝐹𝑡−𝑚𝐿Θ̇𝑡2sin Θ𝑡+𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚]−𝑢𝑝Θ̇𝑡\\n𝑚𝐿\\n𝐿 [4\\n3−𝑚cos 2Θ𝑡\\n𝑀+𝑚]                    (3) \\n \\n𝑋̈𝑡=𝐹𝑡+𝑚𝐿[Θ̇𝑡sinΘ𝑡−Θ̈𝑡cos Θ𝑡]−𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acción se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura está basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenó en un entorno en el cual se \\nrecibe información referente a la posición del péndulo, la velocidad', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='controladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales . \\n \\nÍndice de Términos - Deep Q Learning, péndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI ÓN \\n \\n esde hace  varias  décadas la  respuesta  dinámica y el control \\nde sistemas basados en n-péndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teoría de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemático preciso, además presentan \\nmúltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raíz de estos \\nproblemas surge la motivación de implementar  otras técnicas  como el  \\naprendizaje reforzado y aplicarlas al ámbito del control , debido a l', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='qué acción tomar bajo qué circunstancias , este algoritmo aprende el Implementación de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinámicos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumialán  Borja , C. H. Rodríguez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingeniería en Automatización  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el péndulo invertido estable.', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='no lineal ( péndulo invertido ) como el entorno de interacción para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acción de acuerdo con el estado del sistema, \\nla acción incremental puede acercarse gradualmente a la \\nestrategia óptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacción . \\nPara lograr la convergencia del movimiento de un péndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicación del aprendizaje automático al \\ncontrol de sistemas complejos y caóticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales .', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(query=\"Quienes son los autores?\", k=5, search_type=\"similarity\", filters=\"session eq 'test_001'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7f11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8d359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a9e762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_search: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings_azure.embed_query,\n",
    "    fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4dfdf4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinámica del sistema se \\npresentan en la ecuación 3 y 4. \\n \\nΘ̈𝑡=𝑔sinΘ𝑡 + cos Θ𝑡 [−𝐹𝑡−𝑚𝐿Θ̇𝑡2sin Θ𝑡+𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚]−𝑢𝑝Θ̇𝑡\\n𝑚𝐿\\n𝐿 [4\\n3−𝑚cos 2Θ𝑡\\n𝑀+𝑚]                    (3) \\n \\n𝑋̈𝑡=𝐹𝑡+𝑚𝐿[Θ̇𝑡sinΘ𝑡−Θ̈𝑡cos Θ𝑡]−𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acción se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura está basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenó en un entorno en el cual se \\nrecibe información referente a la posición del péndulo, la velocidad', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='controladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales . \\n \\nÍndice de Términos - Deep Q Learning, péndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI ÓN \\n \\n esde hace  varias  décadas la  respuesta  dinámica y el control \\nde sistemas basados en n-péndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teoría de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemático preciso, además presentan \\nmúltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raíz de estos \\nproblemas surge la motivación de implementar  otras técnicas  como el  \\naprendizaje reforzado y aplicarlas al ámbito del control , debido a l', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='qué acción tomar bajo qué circunstancias , este algoritmo aprende el Implementación de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinámicos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumialán  Borja , C. H. Rodríguez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingeniería en Automatización  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el péndulo invertido estable.', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='no lineal ( péndulo invertido ) como el entorno de interacción para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acción de acuerdo con el estado del sistema, \\nla acción incremental puede acercarse gradualmente a la \\nestrategia óptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacción . \\nPara lograr la convergencia del movimiento de un péndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicación del aprendizaje automático al \\ncontrol de sistemas complejos y caóticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales .', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_search.similarity_search(query=\"Quienes son los autores?\", k=5, search_type=\"similarity\", filters=\"session eq 'test_001'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8466a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46c50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4389948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec428582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21ff4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1419b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f96db94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever_AzureCognitiveSearch_SessionFilter(BaseRetriever):\n",
    "    \n",
    "    AzureCognitiveSearch_vectordb: AzureSearch\n",
    "    session_filter: str\n",
    "    k: int\n",
    "    \n",
    "    def __init__(self, AzureCognitiveSearch_vectordb, session_filter, k):\n",
    "        \n",
    "        super().__init__(AzureCognitiveSearch_vectordb=AzureCognitiveSearch_vectordb, session_filter=session_filter, k=k)\n",
    "        \n",
    "        \n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \n",
    "        documents =  self.AzureCognitiveSearch_vectordb.similarity_search(query=query, k=self.k, search_type=\"similarity\", filters=f\"session eq '{self.session_filter}'\")\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac0557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cc658e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_CognitiveSearch_retriever = CustomRetriever_AzureCognitiveSearch_SessionFilter(vector_store_search, 'test_001', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e27b7797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinámica del sistema se \\npresentan en la ecuación 3 y 4. \\n \\nΘ̈𝑡=𝑔sinΘ𝑡 + cos Θ𝑡 [−𝐹𝑡−𝑚𝐿Θ̇𝑡2sin Θ𝑡+𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚]−𝑢𝑝Θ̇𝑡\\n𝑚𝐿\\n𝐿 [4\\n3−𝑚cos 2Θ𝑡\\n𝑀+𝑚]                    (3) \\n \\n𝑋̈𝑡=𝐹𝑡+𝑚𝐿[Θ̇𝑡sinΘ𝑡−Θ̈𝑡cos Θ𝑡]−𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acción se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura está basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenó en un entorno en el cual se \\nrecibe información referente a la posición del péndulo, la velocidad', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='controladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales . \\n \\nÍndice de Términos - Deep Q Learning, péndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI ÓN \\n \\n esde hace  varias  décadas la  respuesta  dinámica y el control \\nde sistemas basados en n-péndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teoría de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemático preciso, además presentan \\nmúltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raíz de estos \\nproblemas surge la motivación de implementar  otras técnicas  como el  \\naprendizaje reforzado y aplicarlas al ámbito del control , debido a l', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='qué acción tomar bajo qué circunstancias , este algoritmo aprende el Implementación de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinámicos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumialán  Borja , C. H. Rodríguez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingeniería en Automatización  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el péndulo invertido estable.', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='no lineal ( péndulo invertido ) como el entorno de interacción para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acción de acuerdo con el estado del sistema, \\nla acción incremental puede acercarse gradualmente a la \\nestrategia óptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacción . \\nPara lograr la convergencia del movimiento de un péndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicación del aprendizaje automático al \\ncontrol de sistemas complejos y caóticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales .', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0, 'session': 'test_001'})]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_CognitiveSearch_retriever.get_relevant_documents(\"Quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf6187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af97f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "590a9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_azure_chatgpt, chain_type=chain_type, retriever=custom_CognitiveSearch_retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9ba0148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los autores son C. H. Cristancho Toloza, J. A. Tumialán Borja, C. H. Rodríguez Garavito y D. J. Lancheros Cuestas.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"quienes son los autores?\"\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35190f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8af2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159034a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d172c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae7a524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d77f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1d29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea3514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624aa2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f365fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b231cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637450b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431e50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61758082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f069e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aeb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a1da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
