{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840baf44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.0.299\n",
      "  Downloading langchain-0.0.299-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting anyio<4.0\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (2.8.6)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (8.2.3)\n",
      "Collecting langsmith<0.1.0,>=0.0.38\n",
      "  Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (3.8.5)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (2.31.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (1.26.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (0.5.14)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (1.10.12)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/site-packages (from langchain==0.0.299) (2.0.21)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (23.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.299) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.299) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.299) (1.1.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (3.20.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.299) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.299) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.299) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.299) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.299) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (1.0.0)\n",
      "Installing collected packages: jsonpatch, anyio, langsmith, langchain\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.187\n",
      "    Uninstalling langchain-0.0.187:\n",
      "      Successfully uninstalled langchain-0.0.187\n",
      "Successfully installed anyio-3.7.1 jsonpatch-1.33 langchain-0.0.299 langsmith-0.0.41\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting azure-search-documents==11.4.0b8\n",
      "  Downloading azure_search_documents-11.4.0b8-py3-none-any.whl (305 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m305.6/305.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.10/site-packages (from azure-search-documents==11.4.0b8) (0.6.1)\n",
      "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.10/site-packages (from azure-search-documents==11.4.0b8) (1.1.28)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/site-packages (from azure-search-documents==11.4.0b8) (1.29.4)\n",
      "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b8) (2023.7.22)\n",
      "Installing collected packages: azure-search-documents\n",
      "  Attempting uninstall: azure-search-documents\n",
      "    Found existing installation: azure-search-documents 11.4.0b3\n",
      "    Uninstalling azure-search-documents-11.4.0b3:\n",
      "      Successfully uninstalled azure-search-documents-11.4.0b3\n",
      "Successfully installed azure-search-documents-11.4.0b8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain==0.0.299\n",
    "!pip install -U azure-search-documents==11.4.0b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "537297d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99542a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802fd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    ScoringProfile,\n",
    "    TextWeights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2743f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b52efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_type = \"azure\"\n",
    "api_base_url = \"https://openaidemonubiral.openai.azure.com/\"\n",
    "api_version = \"2023-03-15-preview\"\n",
    "azure_api_key = \"ff5c606c134e4d1dae3426a412df834a\"\n",
    "\n",
    "openai.api_type = api_type\n",
    "openai.api_base = api_base_url\n",
    "openai.api_version = api_version\n",
    "openai.api_key = azure_api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = api_base_url\n",
    "os.environ[\"OPENAI_API_KEY\"] = azure_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f6b01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/langchain/embeddings/openai.py:217: UserWarning: WARNING! deployment_id is not default parameter.\n",
      "                    deployment_id was transferred to model_kwargs.\n",
      "                    Please confirm that deployment_id is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings_azure = OpenAIEmbeddings(deployment_id=\"text-embedding-ada-002\", chunk_size=1)\n",
    "\n",
    "llm_azure_chatgpt = AzureChatOpenAI(\n",
    "    deployment_name=\"nubiral-lab-01\", \n",
    "    temperature=0, \n",
    "    openai_api_version=api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6f444d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding = embeddings_azure.embed_query('test de emmbeding')\n",
    "len(test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b07b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c081f4",
   "metadata": {},
   "source": [
    "# TEST BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c450dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_address: str = \"https://cognitive-searchdemo.search.windows.net\"\n",
    "vector_store_password: str = \"Ndqr05h9fitPrfEh9GxNBb2vsvelw9oUgIew1VpUvYAzSeD2n9Zi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9db543",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name: str = \"vector-demo-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd92763",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_cognitive_search: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings_azure.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041891d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19dff406",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "pages = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1edede92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add799f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eea7d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YTUzODExMTAtMjQwNy00OThhLTg2MDEtNDVjOTQxMzRhMzUx',\n",
       " 'YzkzYmUxZTAtZjdlMC00MmE2LTk5ZWItMWY5MDg2OGI5ZWNi',\n",
       " 'N2Q1ZGE1MDctYWM1YS00MGZhLWI1MTEtNGExZjczODZhZTdk',\n",
       " 'MDZjYTdkYTItZWIwNS00ZDlhLTkzZWMtMDBjM2VjNzNhY2Y4',\n",
       " 'NmM4NzQ0MjAtMjljYS00MDJiLTg4OTktN2IwNThiMjg0NzYy',\n",
       " 'MjI1M2Y0ZTAtYTQ0YS00ODcxLWJhNmItMTA1ZTY3MjgwMzAy',\n",
       " 'Yjg1YTkxMjEtZmRlNS00NjM0LTliMzUtODQ1MTdlNDFlZGM2',\n",
       " 'YjcwZDEyMjgtZGMxMC00NDNiLWIzZTItN2IxNDM3OTk0ODdk',\n",
       " 'Mzc0MjYwYWYtMzU5Ni00YjRhLThkZDctMjk4NTNjYzFkYmZl',\n",
       " 'MzNjMmJkMGMtYzYzMi00ZDAyLWEzMjAtNGNiYzg2Y2MyOTQ3',\n",
       " 'ODM5NTBkNGUtNThmMC00NDU2LTkwYjAtZDVkMDY1ZTQ2N2Yy',\n",
       " 'OTRlZDk1OTAtNGJiYy00NGQ5LTk2MjQtZWYwZWY0MDRlMjkz',\n",
       " 'YTZkOTE0NWItMDgwMi00NWM1LTg2NTUtY2RmY2I3NDk3NWZh',\n",
       " 'ZDliY2JkNjktOTk5Ni00ZWY5LWIzZWQtM2EyNGQ0YWE0MGVj',\n",
       " 'MjQwOTYxNGQtYWYxNy00ZjJjLTkyZjQtNzgzNjgzMTYyODNk',\n",
       " 'N2YyZjM0NTYtNjRhMi00OWZkLWJiY2EtZGZlZTk3Y2FiZDk4',\n",
       " 'NzYyOTk1YWEtYTgyZS00NmUxLWE0ZmUtODgyYmQ1NzdhNDEx',\n",
       " 'YzkwMzBmZWQtYjI3Ni00ZDRlLWIwZmYtNmRhOTBkYzhlYjNm',\n",
       " 'MThmOTA1ZGEtMDQwMC00OGVmLWFmODUtM2QxNWJiZTdkNjE0',\n",
       " 'Y2ZjMWJkNDEtMmIyMC00MmNhLTlmNzktYWQ0Nzg2OTJhNWEz',\n",
       " 'NWU1NjVkYTktMDI1Ni00NGVjLWIwYWItOWU2Njk2Y2ZhYTk3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_cognitive_search.add_documents(documents=pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2bf91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7da1f0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='controladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales . \\n \\nndice de T茅rminos - Deep Q Learning, p茅ndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI N \\n \\n esde hace  varias  d茅cadas la  respuesta  din谩mica y el control \\nde sistemas basados en n-p茅ndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teor铆a de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matem谩tico preciso, adem谩s presentan \\nm煤ltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a ra铆z de estos \\nproblemas surge la motivaci贸n de implementar  otras t茅cnicas  como el  \\naprendizaje reforzado y aplicarlas al 谩mbito del control , debido a l', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='qu茅 acci贸n tomar bajo qu茅 circunstancias , este algoritmo aprende el Implementaci贸n de aprendizaje por refuerzo \\nprofundo para el control de sistemas din谩micos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumial谩n  Borja , C. H. Rodr铆guez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingenier铆a en Automatizaci贸n  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evoluci贸n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el p茅ndulo invertido estable.', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la din谩mica del sistema se \\npresentan en la ecuaci贸n 3 y 4. \\n \\n=sin + cos  [桂♀课2sin +sgn \\n+]\\n\\n [4\\n3cos 2\\n+]                    (3) \\n \\n=桂+[sin♀cos ]sgn \\n+                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acci贸n se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura est谩 basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entren贸 en un entorno en el cual se \\nrecibe informaci贸n referente a la posici贸n del p茅ndulo, la velocidad', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='no lineal ( p茅ndulo invertido ) como el entorno de interacci贸n para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acci贸n de acuerdo con el estado del sistema, \\nla acci贸n incremental puede acercarse gradualmente a la \\nestrategia 贸ptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacci贸n . \\nPara lograr la convergencia del movimiento de un p茅ndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicaci贸n del aprendizaje autom谩tico al \\ncontrol de sistemas complejos y ca贸ticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonom铆a del agente o sistema, fren te a los \\ncontroladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales .', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_cognitive_search.similarity_search(\n",
    "    query=\"quienes son los autores?\",\n",
    "    k=5,\n",
    "    search_type=\"similarity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbe5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fed4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bbfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c660abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_db = vector_store_cognitive_search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cfa4b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='entorno.  \\n \\n \\nFig. 5. Evoluci贸n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el p茅ndulo invertido estable.', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='controladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales . \\n \\nndice de T茅rminos - Deep Q Learning, p茅ndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI N \\n \\n esde hace  varias  d茅cadas la  respuesta  din谩mica y el control \\nde sistemas basados en n-p茅ndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teor铆a de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matem谩tico preciso, adem谩s presentan \\nm煤ltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a ra铆z de estos \\nproblemas surge la motivaci贸n de implementar  otras t茅cnicas  como el  \\naprendizaje reforzado y aplicarlas al 谩mbito del control , debido a l', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='no lineal ( p茅ndulo invertido ) como el entorno de interacci贸n para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acci贸n de acuerdo con el estado del sistema, \\nla acci贸n incremental puede acercarse gradualmente a la \\nestrategia 贸ptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacci贸n . \\nPara lograr la convergencia del movimiento de un p茅ndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicaci贸n del aprendizaje autom谩tico al \\ncontrol de sistemas complejos y ca贸ticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonom铆a del agente o sistema, fren te a los \\ncontroladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales .', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la din谩mica del sistema se \\npresentan en la ecuaci贸n 3 y 4. \\n \\n=sin + cos  [桂♀课2sin +sgn \\n+]\\n\\n [4\\n3cos 2\\n+]                    (3) \\n \\n=桂+[sin♀cos ]sgn \\n+                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acci贸n se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura est谩 basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entren贸 en un entorno en el cual se \\nrecibe informaci贸n referente a la posici贸n del p茅ndulo, la velocidad', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='cabe resaltar que en todas las arquitecturas probadas cada capa oculta \\ncuenta con 24 neuronas.  \\n \\n TABLA V \\nARQUITECTURAS EMPLEADAS  \\n Capas ocultas  Cantidad de parametros  \\nModelo 1  1 170 \\nModelo 2  2 770 \\nModelo 3  3 1370  \\nModelo 4  4 1970  \\n \\n   En la figura 4 se presenta de  modo ejemplo una representaci贸n \\ngr谩fica del modelo 2 , con cuatro entradas y dos salidas.  \\n \\n \\nFig. 4. Diagrama de la red neuronal implementada  en el modelo 2 . \\nIV. RESULTADOS  PARCIALES  \\n   Se obtuvo 4 modelos los cuales fueron sometidos a un entrenamiento  \\nde 300 episodios en los cuales los modelos interactuaron con el \\nentorn o, la evoluci贸n del rendimiento de los  modelo s puede ser \\nobservada en la figura 5, en el cual se ve c贸mo  los modelo s mejora n \\ncontinuamente  el valor de recompensa a medida que son expuesto s al \\nentorno.  \\n \\n \\nFig. 5. Evoluci贸n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_db.get_relevant_documents(\"quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708ac8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900910af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a8ee7a",
   "metadata": {},
   "source": [
    "# TEST BUSQUEDA HIBRIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "656254a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_address: str = \"https://cognitive-searchdemo.search.windows.net\"\n",
    "vector_store_password: str = \"Ndqr05h9fitPrfEh9GxNBb2vsvelw9oUgIew1VpUvYAzSeD2n9Zi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "912493ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name: str = \"document-session-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdaaffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcd827ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        filterable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(embeddings_azure.embed_query('test de emmbeding')),\n",
    "        vector_search_configuration=\"default\",\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    # Additional field to store the session\n",
    "    SearchableField(\n",
    "        name=\"session\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        filterable=True,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2e48b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings_azure.embed_query,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad2fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f68369c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "pages = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e92a3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_to_document_metadata(documents, field_name, field_value):\n",
    "    \n",
    "    for document in documents:\n",
    "        \n",
    "        document.metadata[field_name] = field_value\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e322852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = add_data_to_document_metadata(pages, 'session', 'test_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc843ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='no lineal ( p茅ndulo invertido ) como el entorno de interacci贸n para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acci贸n de acuerdo con el estado del sistema, \\nla acci贸n incremental puede acercarse gradualmente a la \\nestrategia 贸ptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacci贸n . \\nPara lograr la convergencia del movimiento de un p茅ndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicaci贸n del aprendizaje autom谩tico al \\ncontrol de sistemas complejos y ca贸ticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonom铆a del agente o sistema, fren te a los \\ncontroladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales .', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28f53942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZmFiM2ZjOTMtMThjNS00MDVlLWJmMmYtZTUwOTY5ZDFjN2Fk',\n",
       " 'MTMxMDVmYzMtM2Y1NS00NGI3LTg3MGQtMWMzMGVhMzRjNjRh',\n",
       " 'ZTRkNmFhOTItMzU4Zi00ZDhlLWExOTUtM2RjYzAzMzA5YWM3',\n",
       " 'ZDM1OTI2NTEtOTgyOC00MjU0LTkxOTAtNjUzNjZmZTI2ZmM3',\n",
       " 'OWIyZmIzMzAtYmM1OC00NzNmLTk5YzMtODViMWY5YzU0NTkz',\n",
       " 'YWI3MjQ3NzItMTNlMi00ZDZhLWIyZWMtODBmNDgyZjljNWU1',\n",
       " 'YWQxZmJmMmQtNGQ3Ni00MjVlLWIxMjUtNzUxNzNiM2I3ZmQy',\n",
       " 'OGNjYjZjMGEtZDJiNC00ZDYxLWE1NDUtMjA3NDJjYTMyN2Qy',\n",
       " 'ZTk4MDQ4ZDAtNGI0Yy00OWVmLTgwMmUtNmFmMTE0MmZhZGEz',\n",
       " 'YjRkMGM4MDgtN2IwMi00NTg3LWFhZDYtOTAzZTFhNzE4YjFl',\n",
       " 'ZDRlMzE0ZjAtNjE2NS00MjNlLTk5ZjEtNmUyOTkyYmQ1YjUy',\n",
       " 'NDczZDdmNjItZjcwYy00YTc4LWFjMTctMmVhOTc0ZDc3ZTY3',\n",
       " 'MjNlZDk0NGEtM2JmZC00NWY0LTg1MmItZDg1NzhiNmRiMTZk',\n",
       " 'MDc4ZjIyMzQtNGI1Ny00N2UyLTllODctMjg4NTA4NjNmMGFm',\n",
       " 'ODc1YmY0NDUtYmFmNy00YmMwLTgzN2YtMDVjYmY3N2U0ZTJi',\n",
       " 'YmJhY2VkOGUtMDYxZi00NTljLTljYzAtMTVhOTM5NTVhYzNm',\n",
       " 'NmExZTViYTMtY2ZhNC00MzQ5LWE0MTAtMmI4Mzg4YjYyNGJj',\n",
       " 'MWM3MWFiN2EtZGU5MC00MDM0LWFiZjEtOTFhNmY4MTQzYmIx',\n",
       " 'NGEwODFlZjktZDBlNy00ZmZkLWJhNjctMDkzNjY1MDZkZmE1',\n",
       " 'YTE1MWQyZDktMTBhNi00M2YyLWJmNWYtZWMwOWFiNjg3N2Fl',\n",
       " 'ZjEzNWYwNDktY2I3MS00NmUwLWJmZjEtMzNlOGJjNDk3ZTI3']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b5c7bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la din谩mica del sistema se \\npresentan en la ecuaci贸n 3 y 4. \\n \\n=sin + cos  [桂♀课2sin +sgn \\n+]\\n\\n [4\\n3cos 2\\n+]                    (3) \\n \\n=桂+[sin♀cos ]sgn \\n+                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acci贸n se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura est谩 basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entren贸 en un entorno en el cual se \\nrecibe informaci贸n referente a la posici贸n del p茅ndulo, la velocidad', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='controladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales . \\n \\nndice de T茅rminos - Deep Q Learning, p茅ndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI N \\n \\n esde hace  varias  d茅cadas la  respuesta  din谩mica y el control \\nde sistemas basados en n-p茅ndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teor铆a de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matem谩tico preciso, adem谩s presentan \\nm煤ltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a ra铆z de estos \\nproblemas surge la motivaci贸n de implementar  otras t茅cnicas  como el  \\naprendizaje reforzado y aplicarlas al 谩mbito del control , debido a l', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='qu茅 acci贸n tomar bajo qu茅 circunstancias , este algoritmo aprende el Implementaci贸n de aprendizaje por refuerzo \\nprofundo para el control de sistemas din谩micos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumial谩n  Borja , C. H. Rodr铆guez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingenier铆a en Automatizaci贸n  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evoluci贸n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el p茅ndulo invertido estable.', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='no lineal ( p茅ndulo invertido ) como el entorno de interacci贸n para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acci贸n de acuerdo con el estado del sistema, \\nla acci贸n incremental puede acercarse gradualmente a la \\nestrategia 贸ptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacci贸n . \\nPara lograr la convergencia del movimiento de un p茅ndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicaci贸n del aprendizaje autom谩tico al \\ncontrol de sistemas complejos y ca贸ticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonom铆a del agente o sistema, fren te a los \\ncontroladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales .', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(query=\"Quienes son los autores?\", k=5, search_type=\"similarity\", filters=\"session eq 'test_001'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7f11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8d359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a9e762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_search: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings_azure.embed_query,\n",
    "    fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4dfdf4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la din谩mica del sistema se \\npresentan en la ecuaci贸n 3 y 4. \\n \\n=sin + cos  [桂♀课2sin +sgn \\n+]\\n\\n [4\\n3cos 2\\n+]                    (3) \\n \\n=桂+[sin♀cos ]sgn \\n+                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acci贸n se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura est谩 basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entren贸 en un entorno en el cual se \\nrecibe informaci贸n referente a la posici贸n del p茅ndulo, la velocidad', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='controladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales . \\n \\nndice de T茅rminos - Deep Q Learning, p茅ndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI N \\n \\n esde hace  varias  d茅cadas la  respuesta  din谩mica y el control \\nde sistemas basados en n-p茅ndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teor铆a de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matem谩tico preciso, adem谩s presentan \\nm煤ltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a ra铆z de estos \\nproblemas surge la motivaci贸n de implementar  otras t茅cnicas  como el  \\naprendizaje reforzado y aplicarlas al 谩mbito del control , debido a l', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='qu茅 acci贸n tomar bajo qu茅 circunstancias , este algoritmo aprende el Implementaci贸n de aprendizaje por refuerzo \\nprofundo para el control de sistemas din谩micos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumial谩n  Borja , C. H. Rodr铆guez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingenier铆a en Automatizaci贸n  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evoluci贸n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el p茅ndulo invertido estable.', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='no lineal ( p茅ndulo invertido ) como el entorno de interacci贸n para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acci贸n de acuerdo con el estado del sistema, \\nla acci贸n incremental puede acercarse gradualmente a la \\nestrategia 贸ptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacci贸n . \\nPara lograr la convergencia del movimiento de un p茅ndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicaci贸n del aprendizaje autom谩tico al \\ncontrol de sistemas complejos y ca贸ticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonom铆a del agente o sistema, fren te a los \\ncontroladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales .', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_search.similarity_search(query=\"Quienes son los autores?\", k=5, search_type=\"similarity\", filters=\"session eq 'test_001'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8466a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46c50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4389948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec428582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21ff4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1419b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f96db94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever_AzureCognitiveSearch_SessionFilter(BaseRetriever):\n",
    "    \n",
    "    AzureCognitiveSearch_vectordb: AzureSearch\n",
    "    session_filter: str\n",
    "    k: int\n",
    "    \n",
    "    def __init__(self, AzureCognitiveSearch_vectordb, session_filter, k):\n",
    "        \n",
    "        super().__init__(AzureCognitiveSearch_vectordb=AzureCognitiveSearch_vectordb, session_filter=session_filter, k=k)\n",
    "        \n",
    "        \n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \n",
    "        documents =  self.AzureCognitiveSearch_vectordb.similarity_search(query=query, k=self.k, search_type=\"similarity\", filters=f\"session eq '{self.session_filter}'\")\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac0557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cc658e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_CognitiveSearch_retriever = CustomRetriever_AzureCognitiveSearch_SessionFilter(vector_store_search, 'test_001', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e27b7797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la din谩mica del sistema se \\npresentan en la ecuaci贸n 3 y 4. \\n \\n=sin + cos  [桂♀课2sin +sgn \\n+]\\n\\n [4\\n3cos 2\\n+]                    (3) \\n \\n=桂+[sin♀cos ]sgn \\n+                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acci贸n se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura est谩 basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entren贸 en un entorno en el cual se \\nrecibe informaci贸n referente a la posici贸n del p茅ndulo, la velocidad', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='controladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales . \\n \\nndice de T茅rminos - Deep Q Learning, p茅ndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI N \\n \\n esde hace  varias  d茅cadas la  respuesta  din谩mica y el control \\nde sistemas basados en n-p茅ndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teor铆a de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matem谩tico preciso, adem谩s presentan \\nm煤ltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a ra铆z de estos \\nproblemas surge la motivaci贸n de implementar  otras t茅cnicas  como el  \\naprendizaje reforzado y aplicarlas al 谩mbito del control , debido a l', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='qu茅 acci贸n tomar bajo qu茅 circunstancias , este algoritmo aprende el Implementaci贸n de aprendizaje por refuerzo \\nprofundo para el control de sistemas din谩micos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumial谩n  Borja , C. H. Rodr铆guez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingenier铆a en Automatizaci贸n  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evoluci贸n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el p茅ndulo invertido estable.', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 2, 'session': 'test_001'}),\n",
       " Document(page_content='no lineal ( p茅ndulo invertido ) como el entorno de interacci贸n para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje \\npor refuerzo elige una acci贸n de acuerdo con el estado del sistema, \\nla acci贸n incremental puede acercarse gradualmente a la \\nestrategia 贸ptima y luego el agente actualiza la red mediante \\ndiferentes recompensas obten idas en el proceso de interacci贸n . \\nPara lograr la convergencia del movimiento de un p茅ndulo \\ninvertido a su estado de equilibrio en una cantidad razonable  de \\niteraciones. Los resultados indican que los algoritmos realizan \\nentre 100 y 300 iteraciones en prom edio para lograr la \\nconvergencia. Esta aplicaci贸n del aprendizaje autom谩tico al \\ncontrol de sistemas complejos y ca贸ticos refuerzan el enfoque de \\nlos algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonom铆a del agente o sistema, fren te a los \\ncontroladores cl谩sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaci贸n  de las redes \\nneuronales .', metadata={'source': '../Data/Implementaci贸n de aprendizaje por refuerzo profundo para el control de sistemas din谩micos no lineales.pdf', 'page': 0, 'session': 'test_001'})]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_CognitiveSearch_retriever.get_relevant_documents(\"Quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf6187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af97f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "590a9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_azure_chatgpt, chain_type=chain_type, retriever=custom_CognitiveSearch_retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9ba0148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los autores son C. H. Cristancho Toloza, J. A. Tumial谩n Borja, C. H. Rodr铆guez Garavito y D. J. Lancheros Cuestas.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"quienes son los autores?\"\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35190f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8af2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159034a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d172c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae7a524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d77f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1d29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea3514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624aa2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f365fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b231cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637450b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431e50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61758082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f069e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aeb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a1da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
