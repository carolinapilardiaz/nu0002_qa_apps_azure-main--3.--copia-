{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46540f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Redis\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58f5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_type = \"azure\"\n",
    "api_base_url = \"https://openaidemonubiral.openai.azure.com/\"\n",
    "api_version = \"2023-03-15-preview\"\n",
    "azure_api_key = \"ff5c606c134e4d1dae3426a412df834a\"\n",
    "\n",
    "openai.api_type = api_type\n",
    "openai.api_base = api_base_url\n",
    "openai.api_version = api_version\n",
    "openai.api_key = azure_api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = api_base_url\n",
    "os.environ[\"OPENAI_API_KEY\"] = azure_api_key\n",
    "\n",
    "#openai.Deployment.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332be22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_azure = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size=1)\n",
    "\n",
    "llm_azure_chatgpt = AzureChatOpenAI(\n",
    "    deployment_name=\"nubiral-lab-01\", \n",
    "    temperature=0, \n",
    "    openai_api_version=api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e31ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f21e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding = embeddings_azure.embed_query('test de emmbeding')\n",
    "len(test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae1c0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_azure_chatgpt([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ac38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8223000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3569ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myHostname = \"vectordb.eastus2.redisenterprise.cache.azure.net\"\n",
    "myPassword = \"7faRnxC4vBcEWoGNuB9hN6gnXxdYpdRtM5KhcGRq0d8=\"\n",
    "\n",
    "r = redis.StrictRedis(host=myHostname, port=10000,\n",
    "                      password=myPassword, ssl=True)\n",
    "\n",
    "r.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5163dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redis_url = \"rediss://:7faRnxC4vBcEWoGNuB9hN6gnXxdYpdRtM5KhcGRq0d8=@vectordb.eastus2.redisenterprise.cache.azure.net:10000\"\n",
    "r = redis.from_url(url=redis_url)\n",
    "r.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e39f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2739aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "pages = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d9a9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='1 \\nResumen – El aprendizaje por refuerzo profundo, o Deep \\nReinforcement Learning , es uno de los campos de la inteligencia \\nartificial que tiene un potencial uso en los próximos años. Este \\nparadigma p ermite que los algoritmos aprendan de su entorno \\npara lograr objetivos, superando así las limitaciones que se \\nencuentran en los algoritmos tradicionales de aprendizaje \\nautomático  enfocas en  que las máquinas aprendan por sí solas.  Sin \\nembargo, no existe un camino sistemático universal  a la hora de \\nestablecer una metodología adecuada para abordar  los problemas \\ny sus aplicaciones. Este articulo presenta  el uso de los algoritmos \\nDeep Q -Learning  con un buffer de repetición de experiencias  \\nimplementado en  Python utilizando las librerías de Tensorflow . \\nComo primer  paso se establece el modelo  de un sistema dinámico \\nno lineal ( péndulo invertido ) como el entorno de interacción para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1350a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbed2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ed6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_url = \"rediss://:7faRnxC4vBcEWoGNuB9hN6gnXxdYpdRtM5KhcGRq0d8=@vectordb.eastus2.redisenterprise.cache.azure.net:10000\"\n",
    "index_name = 'test_01'\n",
    "\n",
    "rds = Redis.from_documents(pages, embeddings_azure, redis_url=redis_url,  index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb812e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3a350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ac7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "retriever_db = rds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f79f1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='los algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales . \\n \\nÍndice de Términos - Deep Q Learning, péndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI ÓN \\n \\n esde hace  varias  décadas la  respuesta  dinámica y el control \\nde sistemas basados en n-péndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teoría de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemático preciso, además presentan \\nmúltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raíz de estos \\nproblemas surge la motivación de implementar  otras técnicas  como el', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='qué acción tomar bajo qué circunstancias , este algoritmo aprende el Implementación de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinámicos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumialán  Borja , C. H. Rodríguez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingeniería en Automatización  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el péndulo invertido estable.', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinámica del sistema se \\npresentan en la ecuación 3 y 4. \\n \\nΘ̈𝑡=𝑔sinΘ𝑡 + cos Θ𝑡 [−𝐹𝑡−𝑚𝐿Θ̇𝑡2sin Θ𝑡+𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚]−𝑢𝑝Θ̇𝑡\\n𝑚𝐿\\n𝐿 [4\\n3−𝑚cos 2Θ𝑡\\n𝑀+𝑚]                    (3) \\n \\n𝑋̈𝑡=𝐹𝑡+𝑚𝐿[Θ̇𝑡sinΘ𝑡−Θ̈𝑡cos Θ𝑡]−𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acción se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura está basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenó en un entorno en el cual se \\nrecibe información referente a la posición del péndulo, la velocidad', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_db.get_relevant_documents(\"quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41743ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7467911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='los algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales . \\n \\nÍndice de Términos - Deep Q Learning, péndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI ÓN \\n \\n esde hace  varias  décadas la  respuesta  dinámica y el control \\nde sistemas basados en n-péndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teoría de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemático preciso, además presentan \\nmúltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raíz de estos \\nproblemas surge la motivación de implementar  otras técnicas  como el', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='qué acción tomar bajo qué circunstancias , este algoritmo aprende el Implementación de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinámicos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumialán  Borja , C. H. Rodríguez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingeniería en Automatización  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el péndulo invertido estable.', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinámica del sistema se \\npresentan en la ecuación 3 y 4. \\n \\nΘ̈𝑡=𝑔sinΘ𝑡 + cos Θ𝑡 [−𝐹𝑡−𝑚𝐿Θ̇𝑡2sin Θ𝑡+𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚]−𝑢𝑝Θ̇𝑡\\n𝑚𝐿\\n𝐿 [4\\n3−𝑚cos 2Θ𝑡\\n𝑀+𝑚]                    (3) \\n \\n𝑋̈𝑡=𝐹𝑡+𝑚𝐿[Θ̇𝑡sinΘ𝑡−Θ̈𝑡cos Θ𝑡]−𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acción se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura está basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenó en un entorno en el cual se \\nrecibe información referente a la posición del péndulo, la velocidad', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_db.get_relevant_documents(\"quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85998e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d96f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_azure_chatgpt, chain_type=chain_type, retriever=retriever_db, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce958480",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"quienes son los autores?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e6f3c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los autores son C. H. Cristancho Toloza, J. A. Tumialán Borja, C. H. Rodríguez Garavito y D. J. Lancheros Cuestas.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f28db19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='los algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales . \\n \\nÍndice de Términos - Deep Q Learning, péndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI ÓN \\n \\n esde hace  varias  décadas la  respuesta  dinámica y el control \\nde sistemas basados en n-péndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teoría de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemático preciso, además presentan \\nmúltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raíz de estos \\nproblemas surge la motivación de implementar  otras técnicas  como el', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='qué acción tomar bajo qué circunstancias , este algoritmo aprende el Implementación de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinámicos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumialán  Borja , C. H. Rodríguez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingeniería en Automatización  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el péndulo invertido estable.', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinámica del sistema se \\npresentan en la ecuación 3 y 4. \\n \\nΘ̈𝑡=𝑔sinΘ𝑡 + cos Θ𝑡 [−𝐹𝑡−𝑚𝐿Θ̇𝑡2sin Θ𝑡+𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚]−𝑢𝑝Θ̇𝑡\\n𝑚𝐿\\n𝐿 [4\\n3−𝑚cos 2Θ𝑡\\n𝑀+𝑚]                    (3) \\n \\n𝑋̈𝑡=𝐹𝑡+𝑚𝐿[Θ̇𝑡sinΘ𝑡−Θ̈𝑡cos Θ𝑡]−𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acción se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura está basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenó en un entorno en el cual se \\nrecibe información referente a la posición del péndulo, la velocidad', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c936e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43206fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ce6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2413d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only redis\n",
    "\n",
    "redis_url = \"redis://redis_server:6379\"\n",
    "index_name = 'test_01'\n",
    "\n",
    "rds_test = Redis(redis_url=redis_url,  index_name=index_name, embedding_function=embeddings_azure.embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e5eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "retriever_db_test = rds_test.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c6f507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='los algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomía del agente o sistema, fren te a los \\ncontroladores clásicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de información  de las redes \\nneuronales . \\n \\nÍndice de Términos - Deep Q Learning, péndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI ÓN \\n \\n esde hace  varias  décadas la  respuesta  dinámica y el control \\nde sistemas basados en n-péndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teoría de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemático preciso, además presentan \\nmúltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raíz de estos \\nproblemas surge la motivación de implementar  otras técnicas  como el', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='qué acción tomar bajo qué circunstancias , este algoritmo aprende el Implementación de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinámicos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. Tumialán  Borja , C. H. Rodríguez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de Ingeniería en Automatización  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. Evolución de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el péndulo invertido estable.', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinámica del sistema se \\npresentan en la ecuación 3 y 4. \\n \\nΘ̈𝑡=𝑔sinΘ𝑡 + cos Θ𝑡 [−𝐹𝑡−𝑚𝐿Θ̇𝑡2sin Θ𝑡+𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚]−𝑢𝑝Θ̇𝑡\\n𝑚𝐿\\n𝐿 [4\\n3−𝑚cos 2Θ𝑡\\n𝑀+𝑚]                    (3) \\n \\n𝑋̈𝑡=𝐹𝑡+𝑚𝐿[Θ̇𝑡sinΘ𝑡−Θ̈𝑡cos Θ𝑡]−𝑢𝑐sgn 𝑋̇𝑡\\n𝑀+𝑚                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acción se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura está basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenó en un entorno en el cual se \\nrecibe información referente a la posición del péndulo, la velocidad', metadata={'source': '../Data/Implementación de aprendizaje por refuerzo profundo para el control de sistemas dinámicos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_db_test.get_relevant_documents(\"quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c202149",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_azure_chatgpt, chain_type=chain_type, retriever=retriever_db_test, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "430dcd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los autores son C. H. Cristancho Toloza, J. A. Tumialán Borja, C. H. Rodríguez Garavito y D. J. Lancheros Cuestas.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"quienes son los autores?\"\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cae1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff20c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_test.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34efb9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
