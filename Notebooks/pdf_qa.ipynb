{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46540f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Redis\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58f5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_type = \"azure\"\n",
    "api_base_url = \"https://openaidemonubiral.openai.azure.com/\"\n",
    "api_version = \"2023-03-15-preview\"\n",
    "azure_api_key = \"ff5c606c134e4d1dae3426a412df834a\"\n",
    "\n",
    "openai.api_type = api_type\n",
    "openai.api_base = api_base_url\n",
    "openai.api_version = api_version\n",
    "openai.api_key = azure_api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = api_base_url\n",
    "os.environ[\"OPENAI_API_KEY\"] = azure_api_key\n",
    "\n",
    "#openai.Deployment.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332be22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_azure = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size=1)\n",
    "\n",
    "llm_azure_chatgpt = AzureChatOpenAI(\n",
    "    deployment_name=\"nubiral-lab-01\", \n",
    "    temperature=0, \n",
    "    openai_api_version=api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e31ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f21e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding = embeddings_azure.embed_query('test de emmbeding')\n",
    "len(test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae1c0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_azure_chatgpt([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ac38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8223000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3569ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myHostname = \"vectordb.eastus2.redisenterprise.cache.azure.net\"\n",
    "myPassword = \"7faRnxC4vBcEWoGNuB9hN6gnXxdYpdRtM5KhcGRq0d8=\"\n",
    "\n",
    "r = redis.StrictRedis(host=myHostname, port=10000,\n",
    "                      password=myPassword, ssl=True)\n",
    "\n",
    "r.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5163dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redis_url = \"rediss://:7faRnxC4vBcEWoGNuB9hN6gnXxdYpdRtM5KhcGRq0d8=@vectordb.eastus2.redisenterprise.cache.azure.net:10000\"\n",
    "r = redis.from_url(url=redis_url)\n",
    "r.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e39f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2739aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "pages = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d9a9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='1 \\nResumen â€“ El aprendizaje por refuerzo profundo, o Deep \\nReinforcement Learning , es uno de los campos de la inteligencia \\nartificial que tiene un potencial uso en los prÃ³ximos aÃ±os. Este \\nparadigma p ermite que los algoritmos aprendan de su entorno \\npara lograr objetivos, superando asÃ­ las limitaciones que se \\nencuentran en los algoritmos tradicionales de aprendizaje \\nautomÃ¡tico  enfocas en  que las mÃ¡quinas aprendan por sÃ­ solas.  Sin \\nembargo, no existe un camino sistemÃ¡tico universal  a la hora de \\nestablecer una metodologÃ­a adecuada para abordar  los problemas \\ny sus aplicaciones. Este articulo presenta  el uso de los algoritmos \\nDeep Q -Learning  con un buffer de repeticiÃ³n de experiencias  \\nimplementado en  Python utilizando las librerÃ­as de Tensorflow . \\nComo primer  paso se establece el modelo  de un sistema dinÃ¡mico \\nno lineal ( pÃ©ndulo invertido ) como el entorno de interacciÃ³n para \\nel agente de aprendiza je por refuerzo. El agente de aprendizaje', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1350a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbed2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ed6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_url = \"rediss://:7faRnxC4vBcEWoGNuB9hN6gnXxdYpdRtM5KhcGRq0d8=@vectordb.eastus2.redisenterprise.cache.azure.net:10000\"\n",
    "index_name = 'test_01'\n",
    "\n",
    "rds = Redis.from_documents(pages, embeddings_azure, redis_url=redis_url,  index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb812e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3a350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ac7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "retriever_db = rds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f79f1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='los algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomÃ­a del agente o sistema, fren te a los \\ncontroladores clÃ¡sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaciÃ³n  de las redes \\nneuronales . \\n \\nÃndice de TÃ©rminos - Deep Q Learning, pÃ©ndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI Ã“N \\n \\n esde hace  varias  dÃ©cadas la  respuesta  dinÃ¡mica y el control \\nde sistemas basados en n-pÃ©ndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teorÃ­a de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemÃ¡tico preciso, ademÃ¡s presentan \\nmÃºltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raÃ­z de estos \\nproblemas surge la motivaciÃ³n de implementar  otras tÃ©cnicas  como el', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='quÃ© acciÃ³n tomar bajo quÃ© circunstancias , este algoritmo aprende el ImplementaciÃ³n de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinÃ¡micos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. TumialÃ¡n  Borja , C. H. RodrÃ­guez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de IngenierÃ­a en AutomatizaciÃ³n  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. EvoluciÃ³n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el pÃ©ndulo invertido estable.', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinÃ¡mica del sistema se \\npresentan en la ecuaciÃ³n 3 y 4. \\n \\nÎ˜Ìˆğ‘¡=ğ‘”sinÎ˜ğ‘¡ + cos Î˜ğ‘¡ [âˆ’ğ¹ğ‘¡âˆ’ğ‘šğ¿Î˜Ì‡ğ‘¡2sin Î˜ğ‘¡+ğ‘¢ğ‘sgn ğ‘‹Ì‡ğ‘¡\\nğ‘€+ğ‘š]âˆ’ğ‘¢ğ‘Î˜Ì‡ğ‘¡\\nğ‘šğ¿\\nğ¿ [4\\n3âˆ’ğ‘šcos 2Î˜ğ‘¡\\nğ‘€+ğ‘š]                    (3) \\n \\nğ‘‹Ìˆğ‘¡=ğ¹ğ‘¡+ğ‘šğ¿[Î˜Ì‡ğ‘¡sinÎ˜ğ‘¡âˆ’Î˜Ìˆğ‘¡cos Î˜ğ‘¡]âˆ’ğ‘¢ğ‘sgn ğ‘‹Ì‡ğ‘¡\\nğ‘€+ğ‘š                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acciÃ³n se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura estÃ¡ basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenÃ³ en un entorno en el cual se \\nrecibe informaciÃ³n referente a la posiciÃ³n del pÃ©ndulo, la velocidad', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_db.get_relevant_documents(\"quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41743ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7467911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='los algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomÃ­a del agente o sistema, fren te a los \\ncontroladores clÃ¡sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaciÃ³n  de las redes \\nneuronales . \\n \\nÃndice de TÃ©rminos - Deep Q Learning, pÃ©ndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI Ã“N \\n \\n esde hace  varias  dÃ©cadas la  respuesta  dinÃ¡mica y el control \\nde sistemas basados en n-pÃ©ndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teorÃ­a de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemÃ¡tico preciso, ademÃ¡s presentan \\nmÃºltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raÃ­z de estos \\nproblemas surge la motivaciÃ³n de implementar  otras tÃ©cnicas  como el', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='quÃ© acciÃ³n tomar bajo quÃ© circunstancias , este algoritmo aprende el ImplementaciÃ³n de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinÃ¡micos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. TumialÃ¡n  Borja , C. H. RodrÃ­guez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de IngenierÃ­a en AutomatizaciÃ³n  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. EvoluciÃ³n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el pÃ©ndulo invertido estable.', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinÃ¡mica del sistema se \\npresentan en la ecuaciÃ³n 3 y 4. \\n \\nÎ˜Ìˆğ‘¡=ğ‘”sinÎ˜ğ‘¡ + cos Î˜ğ‘¡ [âˆ’ğ¹ğ‘¡âˆ’ğ‘šğ¿Î˜Ì‡ğ‘¡2sin Î˜ğ‘¡+ğ‘¢ğ‘sgn ğ‘‹Ì‡ğ‘¡\\nğ‘€+ğ‘š]âˆ’ğ‘¢ğ‘Î˜Ì‡ğ‘¡\\nğ‘šğ¿\\nğ¿ [4\\n3âˆ’ğ‘šcos 2Î˜ğ‘¡\\nğ‘€+ğ‘š]                    (3) \\n \\nğ‘‹Ìˆğ‘¡=ğ¹ğ‘¡+ğ‘šğ¿[Î˜Ì‡ğ‘¡sinÎ˜ğ‘¡âˆ’Î˜Ìˆğ‘¡cos Î˜ğ‘¡]âˆ’ğ‘¢ğ‘sgn ğ‘‹Ì‡ğ‘¡\\nğ‘€+ğ‘š                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acciÃ³n se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura estÃ¡ basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenÃ³ en un entorno en el cual se \\nrecibe informaciÃ³n referente a la posiciÃ³n del pÃ©ndulo, la velocidad', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_db.get_relevant_documents(\"quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85998e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d96f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_azure_chatgpt, chain_type=chain_type, retriever=retriever_db, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce958480",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"quienes son los autores?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e6f3c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los autores son C. H. Cristancho Toloza, J. A. TumialÃ¡n Borja, C. H. RodrÃ­guez Garavito y D. J. Lancheros Cuestas.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f28db19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='los algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomÃ­a del agente o sistema, fren te a los \\ncontroladores clÃ¡sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaciÃ³n  de las redes \\nneuronales . \\n \\nÃndice de TÃ©rminos - Deep Q Learning, pÃ©ndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI Ã“N \\n \\n esde hace  varias  dÃ©cadas la  respuesta  dinÃ¡mica y el control \\nde sistemas basados en n-pÃ©ndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teorÃ­a de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemÃ¡tico preciso, ademÃ¡s presentan \\nmÃºltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raÃ­z de estos \\nproblemas surge la motivaciÃ³n de implementar  otras tÃ©cnicas  como el', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='quÃ© acciÃ³n tomar bajo quÃ© circunstancias , este algoritmo aprende el ImplementaciÃ³n de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinÃ¡micos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. TumialÃ¡n  Borja , C. H. RodrÃ­guez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de IngenierÃ­a en AutomatizaciÃ³n  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. EvoluciÃ³n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el pÃ©ndulo invertido estable.', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinÃ¡mica del sistema se \\npresentan en la ecuaciÃ³n 3 y 4. \\n \\nÎ˜Ìˆğ‘¡=ğ‘”sinÎ˜ğ‘¡ + cos Î˜ğ‘¡ [âˆ’ğ¹ğ‘¡âˆ’ğ‘šğ¿Î˜Ì‡ğ‘¡2sin Î˜ğ‘¡+ğ‘¢ğ‘sgn ğ‘‹Ì‡ğ‘¡\\nğ‘€+ğ‘š]âˆ’ğ‘¢ğ‘Î˜Ì‡ğ‘¡\\nğ‘šğ¿\\nğ¿ [4\\n3âˆ’ğ‘šcos 2Î˜ğ‘¡\\nğ‘€+ğ‘š]                    (3) \\n \\nğ‘‹Ìˆğ‘¡=ğ¹ğ‘¡+ğ‘šğ¿[Î˜Ì‡ğ‘¡sinÎ˜ğ‘¡âˆ’Î˜Ìˆğ‘¡cos Î˜ğ‘¡]âˆ’ğ‘¢ğ‘sgn ğ‘‹Ì‡ğ‘¡\\nğ‘€+ğ‘š                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acciÃ³n se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura estÃ¡ basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenÃ³ en un entorno en el cual se \\nrecibe informaciÃ³n referente a la posiciÃ³n del pÃ©ndulo, la velocidad', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c936e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43206fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ce6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2413d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only redis\n",
    "\n",
    "redis_url = \"redis://redis_server:6379\"\n",
    "index_name = 'test_01'\n",
    "\n",
    "rds_test = Redis(redis_url=redis_url,  index_name=index_name, embedding_function=embeddings_azure.embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e5eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "retriever_db_test = rds_test.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c6f507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='los algoritmos de aprendizaje en muchos campos donde se \\nrequiere lograr la autonomÃ­a del agente o sistema, fren te a los \\ncontroladores clÃ¡sicos dada la capacidad para generalizar \\nexperiencia y extraer patrones de informaciÃ³n  de las redes \\nneuronales . \\n \\nÃndice de TÃ©rminos - Deep Q Learning, pÃ©ndulo invertido, control \\nno lineal, redes neuronales, inteligencia artificial.  \\n \\nI. INTRODUCCI Ã“N \\n \\n esde hace  varias  dÃ©cadas la  respuesta  dinÃ¡mica y el control \\nde sistemas basados en n-pÃ©ndulos invertidos ha tenido una \\ngran relevancia  en el campo de la teorÃ­a de control  [4]. Los \\ncontroladores tradicionales u otros reguladores se utilizan \\nampliamente debido a su simplicidad y eficiencia. Sin embargo, estos \\nrequieren de un modelo matemÃ¡tico preciso, ademÃ¡s presentan \\nmÃºltiples problemas cuando los modelos presentan una alta no \\nlinealidad y son inestables en el tiempo [5]. Por lo que a raÃ­z de estos \\nproblemas surge la motivaciÃ³n de implementar  otras tÃ©cnicas  como el', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='quÃ© acciÃ³n tomar bajo quÃ© circunstancias , este algoritmo aprende el ImplementaciÃ³n de aprendizaje por refuerzo \\nprofundo para el control de sistemas dinÃ¡micos \\nno lineales  \\nC. H. Cristancho  Toloza , J. A. TumialÃ¡n  Borja , C. H. RodrÃ­guez  Garavito , D. J. Lancheros Cuestas.  \\n \\nFacultad de IngenierÃ­a en AutomatizaciÃ³n  \\nUniversidad de la Salle  \\n(ccristancho43, jtumialan , cerodriguez , dilancheros @unisalle.edu.co ) \\n \\nD', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 0}),\n",
       " Document(page_content='entorno.  \\n \\n \\nFig. 5. EvoluciÃ³n de las recompensas de los modelo s provenientes del entorno, \\ndichas recompensas reflejan la capacidad  en que el modelo es capaz de \\nmantener el pÃ©ndulo invertido estable.', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\nLas ecuaciones  diferenciales  que describen la dinÃ¡mica del sistema se \\npresentan en la ecuaciÃ³n 3 y 4. \\n \\nÎ˜Ìˆğ‘¡=ğ‘”sinÎ˜ğ‘¡ + cos Î˜ğ‘¡ [âˆ’ğ¹ğ‘¡âˆ’ğ‘šğ¿Î˜Ì‡ğ‘¡2sin Î˜ğ‘¡+ğ‘¢ğ‘sgn ğ‘‹Ì‡ğ‘¡\\nğ‘€+ğ‘š]âˆ’ğ‘¢ğ‘Î˜Ì‡ğ‘¡\\nğ‘šğ¿\\nğ¿ [4\\n3âˆ’ğ‘šcos 2Î˜ğ‘¡\\nğ‘€+ğ‘š]                    (3) \\n \\nğ‘‹Ìˆğ‘¡=ğ¹ğ‘¡+ğ‘šğ¿[Î˜Ì‡ğ‘¡sinÎ˜ğ‘¡âˆ’Î˜Ìˆğ‘¡cos Î˜ğ‘¡]âˆ’ğ‘¢ğ‘sgn ğ‘‹Ì‡ğ‘¡\\nğ‘€+ğ‘š                                       (4) \\n \\nLos rangos de los valores de la recompensa  que entrega el entono por \\ncada acciÃ³n se presentan  en la Tabla II.  \\n \\nTABLA I I \\nTABLA DE RECOMPENSAS DEL ENTORNO  \\nRecompensa  Estado  \\n+1 Por mantener el baston ergido y dentro de los \\nparametros estipulados . \\n-100 Por permitir la caida del baston o que el carro se salga \\nde los limites . \\n \\nB.  Arquitectura general  \\n \\n    La arquitectura estÃ¡ basada en un  modelo Deep Q -Learning , una red \\nneuronal densa en conjunto con un buffer de memoria (Experience \\nreplay ) [7], esta red neuronal se entrenÃ³ en un entorno en el cual se \\nrecibe informaciÃ³n referente a la posiciÃ³n del pÃ©ndulo, la velocidad', metadata={'source': '../Data/ImplementaciÃ³n de aprendizaje por refuerzo profundo para el control de sistemas dinÃ¡micos no lineales.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_db_test.get_relevant_documents(\"quienes son los autores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c202149",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_azure_chatgpt, chain_type=chain_type, retriever=retriever_db_test, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "430dcd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los autores son C. H. Cristancho Toloza, J. A. TumialÃ¡n Borja, C. H. RodrÃ­guez Garavito y D. J. Lancheros Cuestas.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"quienes son los autores?\"\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cae1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff20c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_test.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34efb9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
